{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\programdata\\anaconda3\\lib\\site-packages (3.1.1)\n",
      "Requirement already satisfied: py4j==0.10.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyspark) (0.10.9)\n"
     ]
    }
   ],
   "source": [
    "! pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName('session4').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-30CBPS7J:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>session4</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x270d80d2670>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-3024ceb04f50>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_pyspark\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'pyspark.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minferSchema\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf_pyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "df_pyspark=spark.read.csv('pyspark.csv',header=True,inferSchema=True)\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----+\n",
      "|   name| age| exp|\n",
      "+-------+----+----+\n",
      "|veeresh|  25|   4|\n",
      "|sachin |  24|   3|\n",
      "|   asas|  26|   2|\n",
      "| ggajdj|  29|   6|\n",
      "|hdhdjah|null|null|\n",
      "|   ahaj|  30|null|\n",
      "+-------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark1=spark.read.option('header','true').csv('Book1.csv',inferSchema=True)\n",
    "df_pyspark1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "|   name| age|\n",
      "+-------+----+\n",
      "|veeresh|  25|\n",
      "|sachin |  24|\n",
      "|   asas|  26|\n",
      "| ggajdj|  29|\n",
      "|hdhdjah|null|\n",
      "|   ahaj|  30|\n",
      "+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.drop('exp').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|veeresh|\n",
      "|sachin |\n",
      "|   asas|\n",
      "| ggajdj|\n",
      "|hdhdjah|\n",
      "|   ahaj|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.drop('age','exp').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- exp: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|veeresh|\n",
      "|sachin |\n",
      "|   asas|\n",
      "| ggajdj|\n",
      "|hdhdjah|\n",
      "|   ahaj|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select('name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----+\n",
      "|   name| age| exp|\n",
      "+-------+----+----+\n",
      "|veeresh|  25|   4|\n",
      "|sachin |  24|   3|\n",
      "|   asas|  26|   2|\n",
      "| ggajdj|  29|   6|\n",
      "|hdhdjah|null|null|\n",
      "|   ahaj|  30|null|\n",
      "+-------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select('name','age','exp').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.describe of DataFrame[name: string, age: int, exp: int]>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----+\n",
      "|   name| age| exp|\n",
      "+-------+----+----+\n",
      "|veeresh|  25|   4|\n",
      "|sachin |  24|   3|\n",
      "|   asas|  26|   2|\n",
      "| ggajdj|  29|   6|\n",
      "|hdhdjah|null|null|\n",
      "|   ahaj|  30|null|\n",
      "+-------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop(how='all',thresh=1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----+----+\n",
      "|   name| age| exp|exp2|\n",
      "+-------+----+----+----+\n",
      "|veeresh|  25|   4|   6|\n",
      "|sachin |  24|   3|   5|\n",
      "|   asas|  26|   2|   4|\n",
      "| ggajdj|  29|   6|   8|\n",
      "|hdhdjah|null|null|null|\n",
      "|   ahaj|  30|null|null|\n",
      "+-------+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.withColumn('exp2',df_pyspark['exp']+2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----+\n",
      "|   name| age|exp2|\n",
      "+-------+----+----+\n",
      "|veeresh|  25|   4|\n",
      "|sachin |  24|   3|\n",
      "|   asas|  26|   2|\n",
      "| ggajdj|  29|   6|\n",
      "|hdhdjah|null|null|\n",
      "|   ahaj|  30|null|\n",
      "+-------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.withColumnRenamed('exp','exp2').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----+\n",
      "|   name| age| exp|\n",
      "+-------+----+----+\n",
      "|veeresh|  25|   4|\n",
      "|sachin |  24|   3|\n",
      "|   asas|  26|   2|\n",
      "| ggajdj|  29|   6|\n",
      "|hdhdjah|null|null|\n",
      "|   ahaj|  30|null|\n",
      "+-------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.fill('missing values','exp').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = Imputer(\n",
    "    inputCols=['age','exp'], \n",
    "    outputCols=[\"{}_imputed\".format(c) for c in [ 'age', 'exp']]\n",
    "    ).setStrategy(\"mean\")\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark=imputer.fit(df_pyspark).transform(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark=df_pyspark.drop('age','exp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filter operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-----------+\n",
      "|   name|age_imputed|exp_imputed|\n",
      "+-------+-----------+-----------+\n",
      "|veeresh|         25|          4|\n",
      "|sachin |         24|          3|\n",
      "|   asas|         26|          2|\n",
      "| ggajdj|         29|          6|\n",
      "|hdhdjah|         26|          3|\n",
      "|   ahaj|         30|          3|\n",
      "+-------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-----------+\n",
      "|   name|age_imputed|exp_imputed|\n",
      "+-------+-----------+-----------+\n",
      "|veeresh|         25|          4|\n",
      "|sachin |         24|          3|\n",
      "|   asas|         26|          2|\n",
      "|hdhdjah|         26|          3|\n",
      "+-------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter((\"age_imputed<=26\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|veeresh|\n",
      "|sachin |\n",
      "|   asas|\n",
      "|hdhdjah|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter((\"age_imputed<=26\")).select('name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Resolved attribute(s) age_imputed#744 missing from name#38,age_imputed#802,exp_imputed#803 in operator !Filter ((age_imputed#744 <= 26) AND (age_imputed#802 >= 20)). Attribute(s) with the same name appear in the operation: age_imputed. Please check if the right attribute(s) are used.;\n!Filter ((age_imputed#744 <= 26) AND (age_imputed#802 >= 20))\n+- Project [name#38, age_imputed#802, exp_imputed#803]\n   +- Project [name#38, age#39, exp#40, cast(CASE WHEN isnull(cast(age#39 as double)) THEN 26.8 WHEN (cast(age#39 as double) = NaN) THEN 26.8 ELSE cast(age#39 as double) END as int) AS age_imputed#802, cast(CASE WHEN isnull(cast(exp#40 as double)) THEN 3.75 WHEN (cast(exp#40 as double) = NaN) THEN 3.75 ELSE cast(exp#40 as double) END as int) AS exp_imputed#803]\n      +- Relation[name#38,age#39,exp#40] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-86-ac67b20d6dbf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_pyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_pysaprk\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"age_imputed\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m<=\u001b[0m\u001b[1;36m26\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m&\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_pyspark\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"age_imputed\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m>=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mfilter\u001b[1;34m(self, condition)\u001b[0m\n\u001b[0;32m   1715\u001b[0m             \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1716\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1717\u001b[1;33m             \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1718\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1719\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"condition should be string or Column\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    115\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Resolved attribute(s) age_imputed#744 missing from name#38,age_imputed#802,exp_imputed#803 in operator !Filter ((age_imputed#744 <= 26) AND (age_imputed#802 >= 20)). Attribute(s) with the same name appear in the operation: age_imputed. Please check if the right attribute(s) are used.;\n!Filter ((age_imputed#744 <= 26) AND (age_imputed#802 >= 20))\n+- Project [name#38, age_imputed#802, exp_imputed#803]\n   +- Project [name#38, age#39, exp#40, cast(CASE WHEN isnull(cast(age#39 as double)) THEN 26.8 WHEN (cast(age#39 as double) = NaN) THEN 26.8 ELSE cast(age#39 as double) END as int) AS age_imputed#802, cast(CASE WHEN isnull(cast(exp#40 as double)) THEN 3.75 WHEN (cast(exp#40 as double) = NaN) THEN 3.75 ELSE cast(exp#40 as double) END as int) AS exp_imputed#803]\n      +- Relation[name#38,age#39,exp#40] csv\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter((df_pysaprk[\"age_imputed\"]<=26)\n",
    "                  (df_pyspark[\"age_imputed\"]>=20)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
